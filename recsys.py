# -*- coding: utf-8 -*-
"""RecSys.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TzZH03bPVAAc5SzEWWHn2ylsJc2wYUSN
"""

import numpy as np
import pandas as pd
import spacy
import en_core_web_sm
import re
import random
from spacy.lang.en.stop_words import STOP_WORDS
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import sparse

from google.colab import files
uploaded = files.upload()

import io
data = io.BytesIO(uploaded['news.csv'])

df = pd.read_csv(data)
df.head()
df = df[:50]



nlp = spacy.load("en_core_web_sm")
art = df['Summary']

# spacy for everything
def cleaning(text):
      nlp = spacy.load("en_core_web_sm")

      text_lower= text.lower()                                        #convert to lower case
      num_removed = re.sub("\d+", "", text_lower)                     #remove numbers
      removed_lines=re.sub('\n','',num_removed)                       #remove
      removed_html = re.compile(r'<.*?>').sub('', removed_lines)      #remove html tags
      result=removed_html

      doc = nlp(result)
      lemmas = [t.lemma_ for t in doc if t.lemma_ not in STOP_WORDS]

      no_stop = []
      puncs = "!\"#$%&()*+-./:;<=>?@[\]^_`{|}~\n'"
      for lemma in lemmas:
          if lemma[-1] in puncs:
              lemma = lemma[:-1]
          if lemma not in puncs and lemma.isalpha():              
              no_stop.append(lemma)

      cleaned = " ".join(no_stop)
      return cleaned

df['Title'] = df['Title'].apply(lambda x: cleaning(x))
df['Summary'] = df['Summary'].apply(lambda x: cleaning(x))

df['Title']

df['Title'] = df['Title'].apply(lambda x: x.replace('\'',''))
df['Summary'] = df['Summary'].apply(lambda x: x.replace('\'',''))
df['Title'] = df['Title'].apply(lambda x: x.replace('^',''))
df['Summary'] = df['Summary'].apply(lambda x: x.replace('^',''))
df['Title'] = df['Title'].apply(lambda x: x.lower())
df['Summary'] = df['Summary'].apply(lambda x: x.lower())



df = df[['Title','Summary','Date','Link']]
df



"""**CLustering of Articles**"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

documents = df['Summary'].to_list()



def vectorize_texts(list_of_strings):
    print('Performing vectorization and TF/IDF transformation on texts...')
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(list_of_strings)
    transformer = TfidfTransformer(smooth_idf=False)
    tfidf = transformer.fit_transform(X)
    return tfidf



from sklearn.cluster import KMeans
def cluster_texts(tfidf):
    #perform kmeans clustering for range of clusters
    distortions = []
    for i in range(1, 15):
      km = KMeans(n_clusters=i, max_iter = 100, verbose = 0, n_init = 1,init='random',tol=1e-04, random_state=0).fit(tfidf)
      distortions.append(km.inertia_)
    
# plot
    plt.plot(range(1, 15), distortions, marker='o')
    plt.xlabel('Number of clusters')
    plt.ylabel('Distortion')
    plt.show()
    
    return km



from sklearn.feature_extraction.text import TfidfTransformer

#vectorized the list of stemmed documents
documents_vectorized = vectorize_texts(documents)

cluster= cluster_texts(documents_vectorized)

kmeans_df = pd.DataFrame()

kmeans_df['cluster'] = cluster.labels_

kmeans_df['stemmed'] = df['Summary']

kmeans_df['cluster'].value_counts()

kmeans_df

ax = sns.countplot(x= 'cluster', data=kmeans_df)
ax.set_xticklabels(ax.get_xticklabels(), rotation=45)

#function to find the most common words within each cluster
def get_most_common_words(df, df_column, num_words):
    common_words = []
    for i in range(0,12):
        common = Counter(" ".join(df.loc[df_column == i]['stemmed']).split()).most_common(num_words)
        for j in common:
            dict_ = {}
            dict_['cluster'] = i
            dict_['word'] = j[0]
            common_words.append(dict_)
            
    return common_words

from collections import Counter

get_most_common_words(kmeans_df, kmeans_df['cluster'], 25)

Counter(" ".join(kmeans_df.loc[kmeans_df['cluster'] == 2]['stemmed']).split()).most_common(10)

df = pd.concat([df, kmeans_df['cluster']], axis = 1, sort = False)
df



"""**WORD CLOUD & LDA**"""

from wordcloud import WordCloud
# Join the different processed titles together.
long_string = ','.join(list(df['Title'].values))
# Create a WordCloud object
wordcloud = WordCloud(background_color="white", max_words=5000, contour_width=3, contour_color='steelblue')
# Generate a word cloud
wordcloud.generate(long_string)
# Visualize the word cloud
wordcloud.to_image()

# Commented out IPython magic to ensure Python compatibility.
# Load the library with the CountVectorizer method
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline

# Helper function
def plot_10_most_common_words(count_data, count_vectorizer):
    import matplotlib.pyplot as plt
    words = count_vectorizer.get_feature_names()
    total_counts = np.zeros(len(words))
    for t in count_data:
        total_counts+=t.toarray()[0]
    
    count_dict = (zip(words, total_counts))
    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]
    words = [w[0] for w in count_dict]
    counts = [w[1] for w in count_dict]
    x_pos = np.arange(len(words)) 
    
    plt.figure(2, figsize=(15, 15/1.6180))
    plt.subplot(title='10 most common words')
    sns.set_context("notebook", font_scale=1.25, rc={"lines.linewidth": 2.5})
    sns.barplot(x_pos, counts, palette='husl')
    plt.xticks(x_pos, words, rotation=90) 
    plt.xlabel('words')
    plt.ylabel('counts')
    plt.show()
# Initialise the count vectorizer with the English stop words
count_vectorizer = CountVectorizer(stop_words='english')
# Fit and transform the processed titles
count_data = count_vectorizer.fit_transform(df['Title'])
# Visualise the 10 most common words
plot_10_most_common_words(count_data, count_vectorizer)

import warnings
warnings.simplefilter("ignore", DeprecationWarning)
# Load the LDA model from sklearn
from sklearn.decomposition import LatentDirichletAllocation as LDA
 
# Helper function
def print_topics(model, count_vectorizer, n_top_words):
    words = count_vectorizer.get_feature_names()
    for topic_idx, topic in enumerate(model.components_):
        print("\nTopic #%d:" % topic_idx)
        print(" ".join([words[i]
                        for i in topic.argsort()[:-n_top_words - 1:-1]]))
        
# Tweak the two parameters below
number_topics = 10
number_words = 7
# Create and fit the LDA model
lda = LDA(n_components=number_topics, n_jobs=-1)
lda.fit(count_data)
# Print the topics found by the LDA model
print("Topics found via LDA:")
print_topics(lda, count_vectorizer, number_words)

!pip install pyLDAvis

from pyLDAvis import sklearn as sklearn_lda
import pickle 
import pyLDAvis
# Visualize the topics
pyLDAvis.enable_notebook()
LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)
LDAvis_prepared

"""**Creating User Profile and Merge into Article Matrix**"""



Ratings = []
TimeStamp = []
UserID = []
ArticleId = []
VisitId = []
Click = []
#4713 articles are there...
#lets say that there are 10 users...
for i in range(0,50):
  UserID.append(random.randrange(1,11))
  VisitId.append(random.randrange(1,random.randint(2,11)))
  TimeStamp.append(random.randrange(0,11))
  Ratings.append(round(np.random.uniform(0.1, 5),2))
  ArticleId.append(random.randrange(0,50))
  #if np.random.binomial(1,0.5)==0:
  #  s='No'
  #else:
  # s='Yes'
  Click.append(np.random.binomial(1,0.5))

data = {'UserID':UserID,'VisitId':VisitId,'ArticleId':ArticleId,'Click':Click,'Ratings':Ratings,'TimeStamp':TimeStamp}
df1 = pd.DataFrame (data)
df1



"""**DistPLot w.r.t Length of article summary**"""

import seaborn as sns

#create new column for the character length of each article
df['len_article'] = df.Summary.str.len()

sns.distplot(df.len_article)

print('mean:', df.len_article.mean())
print('std:', df.len_article.std())
print('max:', df.len_article.max())
print('min:', df.len_article.min())

df['ArticleId']=df.index

df.head()



df1.head()

article_ratings = pd.merge(df, df1)
article_ratings.head(15)



from sklearn.metrics.pairwise import cosine_similarity

def matrix_factorization(R, P, Q, K, steps=1000, alpha=0.0002, beta=0.02):
    '''
    R: rating matrix
    P: |U| * K (User features matrix)
    Q: |D| * K (Item features matrix)
    K: latent features
    steps: iterations
    alpha: learning rate
    beta: regularization parameter'''
    Q = Q.T

    for step in range(steps):
        for i in range(len(R)):
            for j in range(len(R[i])):
                if R[i][j] > 0:
                    # calculate error
                    eij = R[i][j] - np.dot(P[i,:],Q[:,j])

                    for k in range(K):
                        # calculate gradient with a and beta parameter
                        P[i][k] = P[i][k] + alpha * (2 * eij * Q[k][j] - beta * P[i][k])
                        Q[k][j] = Q[k][j] + alpha * (2 * eij * P[i][k] - beta * Q[k][j])

        eR = np.dot(P,Q)

        e = 0

        for i in range(len(R)):

            for j in range(len(R[i])):

                if R[i][j] > 0:

                    e = e + pow(R[i][j] - np.dot(P[i,:],Q[:,j]), 2)

                    for k in range(K):

                        e = e + (beta/2) * (pow(P[i][k],2) + pow(Q[k][j],2))
        # 0.001: local minimum
        if e < 0.001:

            break

    return P, Q.T

ratings_matrix1 = df1.pivot_table(index=['ArticleId'],columns=['UserID'],values='Ratings').reset_index(drop=True)
ratings_matrix1.fillna( 0, inplace = True )
R=np.array(ratings_matrix1)
R

# N: num of User
N = len(R)
# M: num of Movie
M = len(R[0])
# Num of Features
K = 3

 
P = np.random.rand(N,K)
Q = np.random.rand(M,K)

 

nP, nQ = matrix_factorization(R, P, Q, K)

nR = np.dot(nP, nQ.T)

ratings_matrix2 = pd.DataFrame(nR)
ratings_matrix2 = ratings_matrix2.round(decimals=2)
ratings_matrix2.head(15)



try:
    #user_inp=input('Enter the reference movie title based on which recommendations are to be made: ')
    user_inp="thailand plane cafe help customer pretend sky"
    #user_inp=30
    inp=df[df['Title']==user_inp].index.tolist()
    #print(inp)
    inp=inp[0]
    #inp = user_inp
    df['Score'] = ratings_matrix2.iloc[inp]
    
except:
    print("Sorry, the article is not in the database!")
    
print("Recommended articles based on your choice of ArticleId ",user_inp , df.sort_values( ["Score"], ascending = False )[1:10])











